{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcacdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ktb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for Pyannote: mps\n",
      "Using compute type for Pyannote: float16\n",
      "Using device for Whisper/Alignment: cpu\n",
      "Using compute type for Whisper/Alignment: float32\n",
      "Starting speaker diarization...\n",
      "Speaker diarization complete.\n",
      "\n",
      "--- Diarization Result (DataFrame) ---\n",
      "         start         end     speaker\n",
      "0     0.030969    2.646594  SPEAKER_01\n",
      "1     2.815344    4.131594  SPEAKER_00\n",
      "2     4.435344    6.544719  SPEAKER_00\n",
      "3     7.185969   11.674719  SPEAKER_00\n",
      "4     7.320969    7.725969  SPEAKER_01\n",
      "5    11.708469   14.070969  SPEAKER_01\n",
      "6    13.514094   17.395344  SPEAKER_00\n",
      "7    29.764719   30.372219  SPEAKER_01\n",
      "8    30.810969   32.532219  SPEAKER_01\n",
      "9    32.903469   51.330969  SPEAKER_01\n",
      "10   48.917844   49.373469  SPEAKER_00\n",
      "11   50.419719   51.246594  SPEAKER_00\n",
      "12   51.330969   51.381594  SPEAKER_00\n",
      "13   52.444719   63.430344  SPEAKER_01\n",
      "14   60.612219   60.797844  SPEAKER_00\n",
      "15   64.527219   65.404719  SPEAKER_01\n",
      "16   65.826594   68.307219  SPEAKER_01\n",
      "17   68.442219   69.707844  SPEAKER_01\n",
      "18   69.859719   71.142219  SPEAKER_01\n",
      "19   71.243469   72.002844  SPEAKER_01\n",
      "20   71.445969   71.901594  SPEAKER_00\n",
      "21   72.880344   77.689719  SPEAKER_01\n",
      "22   78.617844   83.545344  SPEAKER_01\n",
      "23   80.288469   80.474094  SPEAKER_00\n",
      "24   84.254094   85.182219  SPEAKER_01\n",
      "25   85.283469   89.856594  SPEAKER_01\n",
      "26   89.299719   89.957844  SPEAKER_00\n",
      "27   89.957844   92.269719  SPEAKER_01\n",
      "28   92.607219   94.969719  SPEAKER_01\n",
      "29   95.374719   99.998469  SPEAKER_01\n",
      "30   95.425344   95.813469  SPEAKER_00\n",
      "31   99.036594   99.509094  SPEAKER_00\n",
      "32  100.504719  102.816594  SPEAKER_01\n",
      "33  103.204719  110.292219  SPEAKER_01\n",
      "34  105.195969  105.516594  SPEAKER_00\n",
      "35  110.595969  111.625344  SPEAKER_01\n",
      "36  111.996594  119.522844  SPEAKER_01\n",
      "37  113.920344  114.190344  SPEAKER_00\n",
      "Number of diarization segments: 38\n",
      "--------------------------------------\n",
      "Loading OpenAI Whisper model on device: cpu...\n",
      "OpenAI Whisper model loaded.\n",
      "Transcribing audio with OpenAI Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ktb/lib/python3.10/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 50%|█████     | 6000/12000 [02:42<02:42, 36.98frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete.\n",
      "Initializing forced alignment service (using load_alignment_model)...\n",
      "Forced alignment model and tokenizer loaded.\n",
      "Loading full audio for alignment...\n",
      "Full audio loaded and processed.\n",
      "Performing forced alignment on transcription segments...\n",
      "    Warning: Could not align segment 'ดิจิ่งโลกเราก็เคยผ่านการสุนพันศ์' (start: 0.00s, end: 2.68s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 42880]\n",
      "    Warning: Could not align segment 'หิน 10 กิโล' (start: 2.68s, end: 4.08s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 22400]\n",
      "    Warning: Could not align segment 'พุ่งมาด้วยความเร็ว 20 กิโลเมตร' (start: 4.34s, end: 6.68s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 37440]\n",
      "    Warning: Could not align segment 'ต่อวินาที' (start: 7.06s, end: 8.66s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 25600]\n",
      "    Warning: Could not align segment 'พัศพิรุณเนี่ย' (start: 8.66s, end: 9.88s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 19520]\n",
      "    Warning: Could not align segment 'ก่อมน้ำลงมาดับไฟให้นะ' (start: 9.88s, end: 11.68s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 28800]\n",
      "    Warning: Could not align segment '- อ๋อ พัศพิรุณยังเห็นใจล้าว! ใช่!' (start: 11.68s, end: 14.08s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 38400]\n",
      "    Warning: Could not align segment 'เอ่อ!' (start: 14.08s, end: 14.92s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 13440]\n",
      "    Warning: Could not align segment 'แต่น้ำนั่นน่ะ คือ น้ำกดซันฟิวริก' (start: 14.92s, end: 17.36s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 39040]\n",
      "    Warning: Could not align segment 'อือางน้ำ' (start: 17.36s, end: 19.82s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 39360]\n",
      "    Warning: Could not align segment 'งานที่ดูเนี่ยผมก็จะดูพวกศาลคดี' (start: 107.36s, end: 110.36s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 48000]\n",
      "    Warning: Could not align segment 'แต่ถ้าฟัง podcast ผมก็จะฟังหา podcast ที่มันเป็นด้านศาละ' (start: 110.36s, end: 116.36s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 96000]\n",
      "    Warning: Could not align segment 'แล้วก็ผมฟังตั้งแต่ผมยังทันตั้งแต่ original' (start: 116.36s, end: 119.36s). Error: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 48000]\n",
      "Alignment complete.\n",
      "Assigning speakers to transcription segments...\n",
      "Speaker assignment complete.\n",
      "\n",
      "--- Speaker-Labeled Transcription ---\n",
      "[SPEAKER_01] 0.00s - 2.68s: ดิจิ่งโลกเราก็เคยผ่านการสุนพันศ์\n",
      "[SPEAKER_00] 2.68s - 4.08s: หิน 10 กิโล\n",
      "[SPEAKER_00] 4.34s - 6.68s: พุ่งมาด้วยความเร็ว 20 กิโลเมตร\n",
      "[SPEAKER_00] 7.06s - 8.66s: ต่อวินาที\n",
      "[SPEAKER_00] 8.66s - 9.88s: พัศพิรุณเนี่ย\n",
      "[SPEAKER_00] 9.88s - 11.68s: ก่อมน้ำลงมาดับไฟให้นะ\n",
      "[SPEAKER_01] 11.68s - 14.08s: - อ๋อ พัศพิรุณยังเห็นใจล้าว! ใช่!\n",
      "[SPEAKER_00] 14.08s - 14.92s: เอ่อ!\n",
      "[SPEAKER_00] 14.92s - 17.36s: แต่น้ำนั่นน่ะ คือ น้ำกดซันฟิวริก\n",
      "[SPEAKER_00] 17.36s - 19.82s: อือางน้ำ\n",
      "[SPEAKER_01] 107.36s - 110.36s: งานที่ดูเนี่ยผมก็จะดูพวกศาลคดี\n",
      "[SPEAKER_01] 110.36s - 116.36s: แต่ถ้าฟัง podcast ผมก็จะฟังหา podcast ที่มันเป็นด้านศาละ\n",
      "[SPEAKER_01] 116.36s - 119.36s: แล้วก็ผมฟังตั้งแต่ผมยังทันตั้งแต่ original\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper # The official OpenAI Whisper library\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "# Revert to the documented import structure\n",
    "from ctc_forced_aligner import (\n",
    "    load_audio as ctc_load_audio, # Renamed to avoid conflict with torchaudio.load\n",
    "    load_alignment_model,\n",
    "    generate_emissions,\n",
    "    preprocess_text,\n",
    "    get_alignments,\n",
    "    get_spans,\n",
    "    postprocess_results,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please set the HF_TOKEN environment variable.\")\n",
    "\n",
    "audio_file = \"output1.wav\"\n",
    "\n",
    "if not os.path.exists(audio_file):\n",
    "    raise FileNotFoundError(f\"The audio file was not found at: {audio_file}\")\n",
    "\n",
    "# Determine device for Pyannote and Whisper/Alignment models\n",
    "if torch.backends.mps.is_available():\n",
    "    device_pyannote = \"mps\"\n",
    "    compute_type_pyannote = \"float16\"\n",
    "\n",
    "    device_whisper_align = \"cpu\" # Still force CPU for Whisper/Alignment on MPS\n",
    "    compute_type_whisper_align = \"float32\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_pyannote = \"cuda\"\n",
    "    compute_type_pyannote = \"float16\"\n",
    "    device_whisper_align = \"cuda\"\n",
    "    compute_type_whisper_align = \"float16\"\n",
    "else:\n",
    "    device_pyannote = \"cpu\"\n",
    "    compute_type_pyannote = \"float32\"\n",
    "    device_whisper_align = \"cpu\"\n",
    "    compute_type_whisper_align = \"float32\"\n",
    "\n",
    "print(f\"Using device for Pyannote: {device_pyannote}\")\n",
    "print(f\"Using compute type for Pyannote: {compute_type_pyannote}\")\n",
    "print(f\"Using device for Whisper/Alignment: {device_whisper_align}\")\n",
    "print(f\"Using compute type for Whisper/Alignment: {compute_type_whisper_align}\")\n",
    "\n",
    "print(\"Starting speaker diarization...\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "diarization_pipeline.to(torch.device(device_pyannote))\n",
    "diarization = diarization_pipeline(audio_file)\n",
    "print(\"Speaker diarization complete.\")\n",
    "\n",
    "data = []\n",
    "for segment, track, speaker in diarization.itertracks(yield_label=True):\n",
    "    data.append({\n",
    "        'start': segment.start,\n",
    "        'end': segment.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "diarization_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\n--- Diarization Result (DataFrame) ---\")\n",
    "print(diarization_df)\n",
    "print(f\"Number of diarization segments: {len(diarization_df)}\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "# 2. Transcription with OpenAI Whisper\n",
    "print(f\"Loading OpenAI Whisper model on device: {device_whisper_align}...\")\n",
    "model = whisper.load_model(\"medium\", device=device_whisper_align)\n",
    "print(\"OpenAI Whisper model loaded.\")\n",
    "\n",
    "print(\"Transcribing audio with OpenAI Whisper...\")\n",
    "transcription_raw_result = model.transcribe(audio_file, language=\"th\", verbose=False)\n",
    "transcription_result = {'segments': []}\n",
    "if 'segments' in transcription_raw_result:\n",
    "    for segment in transcription_raw_result['segments']:\n",
    "        transcription_result['segments'].append({\n",
    "            'text': segment['text'],\n",
    "            'start': segment['start'],\n",
    "            'end': segment['end']\n",
    "        })\n",
    "print(\"Transcription complete.\")\n",
    "\n",
    "# print(\"\\n--- Transcription Result (raw Whisper output) ---\")\n",
    "# print(f\"Number of transcription segments: {len(transcription_result['segments'])}\")\n",
    "# for i, seg in enumerate(transcription_result['segments']):\n",
    "#     print(f\"  Segment {i+1}: Start={seg['start']:.2f}, End={seg['end']:.2f}, Text='{seg['text'].strip()}'\")\n",
    "# print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "# 3. Forced Alignment with ctc-forced-aligner\n",
    "print(\"Initializing forced alignment service (using load_alignment_model)...\")\n",
    "try:\n",
    "    alignment_model, alignment_tokenizer = load_alignment_model(\n",
    "        device_whisper_align,\n",
    "        dtype=torch.float16 if device_whisper_align == \"cuda\" else torch.float32,\n",
    "    )\n",
    "    print(\"Forced alignment model and tokenizer loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading alignment model: {e}\")\n",
    "    print(\"Please ensure the ctc-forced-aligner library is correctly installed from GitHub and dependencies are met.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "aligned_result_segments = []\n",
    "\n",
    "print(\"Loading full audio for alignment...\")\n",
    "try:\n",
    "    # Using torchaudio.load as it's generally more robust\n",
    "    full_audio_waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    if sample_rate != 16000:\n",
    "        print(f\"Resampling audio from {sample_rate}Hz to 16000Hz...\")\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000).to(device_whisper_align)\n",
    "        full_audio_waveform = resampler(full_audio_waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    # Ensure it's mono and on the correct device for emissions generation\n",
    "    full_audio_waveform = full_audio_waveform.mean(dim=0, keepdim=True).to(device_whisper_align)\n",
    "\n",
    "    # ctc-forced-aligner's generate_emissions expects audio waveform\n",
    "    print(\"Full audio loaded and processed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading/processing audio with torchaudio: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"Performing forced alignment on transcription segments...\")\n",
    "for i, segment in enumerate(transcription_result['segments']):\n",
    "    segment_text = segment['text']\n",
    "    segment_start = segment['start']\n",
    "    segment_end = segment['end']\n",
    "\n",
    "    # print(f\"  Aligning segment {i+1}: {segment_start:.2f}s - {segment_end:.2f}s, Text: '{segment_text.strip()}'\")\n",
    "\n",
    "    # Extract the audio chunk for the current segment\n",
    "    start_sample_abs = int(segment_start * sample_rate)\n",
    "    end_sample_abs = int(segment_end * sample_rate)\n",
    "\n",
    "    start_sample_abs = max(0, start_sample_abs)\n",
    "    end_sample_abs = min(full_audio_waveform.shape[-1], end_sample_abs)\n",
    "\n",
    "    if start_sample_abs >= end_sample_abs:\n",
    "        print(f\"    Warning: Skipping empty or invalid audio chunk for segment {i+1}.\")\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': []\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    current_audio_chunk_tensor = full_audio_waveform[:, start_sample_abs:end_sample_abs]\n",
    "\n",
    "    try:\n",
    "        # Generate emissions for the current audio chunk\n",
    "        emissions, stride = generate_emissions(\n",
    "            alignment_model,\n",
    "            current_audio_chunk_tensor,\n",
    "            batch_size=1 # Process one chunk at a time\n",
    "        )\n",
    "\n",
    "        # Preprocess the text\n",
    "        tokens_starred, text_starred = preprocess_text(\n",
    "            segment_text,\n",
    "            romanize=True, # Set to True if using models that require romanization (e.g., multilingual)\n",
    "            language=\"th\"\n",
    "        )\n",
    "\n",
    "        # Get alignments\n",
    "        segments, scores, blank_token = get_alignments(\n",
    "            emissions,\n",
    "            tokens_starred,\n",
    "            alignment_tokenizer,\n",
    "        )\n",
    "\n",
    "        # Get spans (word-level details)\n",
    "        spans = get_spans(\n",
    "            segments,\n",
    "            text_starred,\n",
    "            offset=0, # This offset is relative to the *chunk*, we'll add segment_start later\n",
    "            blank_token=blank_token\n",
    "        )\n",
    "\n",
    "        # Postprocess results to get word-level timings\n",
    "        aligned_words_raw = postprocess_results(\n",
    "            spans,\n",
    "            score_threshold=0.8 # Adjust threshold if needed\n",
    "        )\n",
    "\n",
    "        aligned_words = []\n",
    "        for word_info in aligned_words_raw:\n",
    "            # Add the original segment's start time to the word timings\n",
    "            word_info['start'] += segment_start\n",
    "            word_info['end'] += segment_start\n",
    "            aligned_words.append(word_info)\n",
    "\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': aligned_words\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not align segment '{segment_text}' (start: {segment_start:.2f}s, end: {segment_end:.2f}s). Error: {e}\")\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': []\n",
    "        })\n",
    "\n",
    "aligned_result = {'segments': aligned_result_segments}\n",
    "print(\"Alignment complete.\")\n",
    "\n",
    "# print(\"\\n--- Aligned Result ---\")\n",
    "# print(f\"Number of aligned segments: {len(aligned_result['segments'])}\")\n",
    "# for i, seg in enumerate(aligned_result['segments']):\n",
    "#     text_display = \"\".join([word[\"word\"] for word in seg[\"words\"]]).strip() if \"words\" in seg and seg[\"words\"] else seg[\"text\"].strip()\n",
    "#     print(f\"  Segment {i+1}: Start={seg['start']:.2f}, End={seg['end']:.2f}, Text='{text_display}'\")\n",
    "#     if \"words\" in seg and seg[\"words\"]:\n",
    "#         for word in seg[\"words\"]:\n",
    "#             print(f\"    - Word: '{word['word']}', Start={word['start']:.2f}, End={word['end']:.2f}\")\n",
    "# print(\"----------------------\")\n",
    "\n",
    "\n",
    "# 4. Assign Speaker Labels\n",
    "print(\"Assigning speakers to transcription segments...\")\n",
    "result_with_speakers = aligned_result.copy()\n",
    "result_with_speakers['segments'] = []\n",
    "\n",
    "for segment in aligned_result['segments']:\n",
    "    segment_start = segment['start']\n",
    "    segment_end = segment['end']\n",
    "    assigned_speaker = \"Unknown\"\n",
    "\n",
    "    max_overlap = 0\n",
    "    for idx, row in diarization_df.iterrows():\n",
    "        diar_start = row['start']\n",
    "        diar_end = row['end']\n",
    "\n",
    "        overlap_start = max(segment_start, diar_start)\n",
    "        overlap_end = min(segment_end, diar_end)\n",
    "\n",
    "        overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "        if overlap_duration > max_overlap:\n",
    "            max_overlap = overlap_duration\n",
    "            assigned_speaker = row['speaker']\n",
    "\n",
    "    segment['speaker'] = assigned_speaker\n",
    "    result_with_speakers['segments'].append(segment)\n",
    "print(\"Speaker assignment complete.\")\n",
    "\n",
    "\n",
    "# 5. Print the final result\n",
    "print(\"\\n--- Speaker-Labeled Transcription ---\")\n",
    "for segment in result_with_speakers[\"segments\"]:\n",
    "    speaker = segment.get(\"speaker\", \"Unknown Speaker\")\n",
    "    start_time = segment[\"start\"]\n",
    "    end_time = segment[\"end\"]\n",
    "    text = segment[\"text\"].strip()\n",
    "    print(f\"[{speaker}] {start_time:.2f}s - {end_time:.2f}s: {text}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
