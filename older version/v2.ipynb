{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcacdc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyannote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwhisper\u001b[39;00m \u001b[38;5;66;03m# The official OpenAI Whisper library\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'whisper'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper # The official OpenAI Whisper library\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "# Revert to the documented import structure\n",
    "from ctc_forced_aligner import (\n",
    "    load_audio as ctc_load_audio, # Renamed to avoid conflict with torchaudio.load\n",
    "    load_alignment_model,\n",
    "    generate_emissions,\n",
    "    preprocess_text,\n",
    "    get_alignments,\n",
    "    get_spans,\n",
    "    postprocess_results,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please set the HF_TOKEN environment variable.\")\n",
    "\n",
    "audio_file = \"output1.wav\"\n",
    "\n",
    "if not os.path.exists(audio_file):\n",
    "    raise FileNotFoundError(f\"The audio file was not found at: {audio_file}\")\n",
    "\n",
    "# Determine device for Pyannote and Whisper/Alignment models\n",
    "if torch.backends.mps.is_available():\n",
    "    device_pyannote = \"mps\"\n",
    "    compute_type_pyannote = \"float16\"\n",
    "\n",
    "    device_whisper_align = \"cpu\" # Still force CPU for Whisper/Alignment on MPS\n",
    "    compute_type_whisper_align = \"float32\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_pyannote = \"cuda\"\n",
    "    compute_type_pyannote = \"float16\"\n",
    "    device_whisper_align = \"cuda\"\n",
    "    compute_type_whisper_align = \"float16\"\n",
    "else:\n",
    "    device_pyannote = \"cpu\"\n",
    "    compute_type_pyannote = \"float32\"\n",
    "    device_whisper_align = \"cpu\"\n",
    "    compute_type_whisper_align = \"float32\"\n",
    "\n",
    "print(f\"Using device for Pyannote: {device_pyannote}\")\n",
    "print(f\"Using compute type for Pyannote: {compute_type_pyannote}\")\n",
    "print(f\"Using device for Whisper/Alignment: {device_whisper_align}\")\n",
    "print(f\"Using compute type for Whisper/Alignment: {compute_type_whisper_align}\")\n",
    "\n",
    "print(\"Starting speaker diarization...\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "diarization_pipeline.to(torch.device(device_pyannote))\n",
    "diarization = diarization_pipeline(audio_file)\n",
    "print(\"Speaker diarization complete.\")\n",
    "\n",
    "data = []\n",
    "for segment, track, speaker in diarization.itertracks(yield_label=True):\n",
    "    data.append({\n",
    "        'start': segment.start,\n",
    "        'end': segment.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "diarization_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\n--- Diarization Result (DataFrame) ---\")\n",
    "print(diarization_df)\n",
    "print(f\"Number of diarization segments: {len(diarization_df)}\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "# 2. Transcription with OpenAI Whisper\n",
    "print(f\"Loading OpenAI Whisper model on device: {device_whisper_align}...\")\n",
    "model = whisper.load_model(\"medium\", device=device_whisper_align)\n",
    "print(\"OpenAI Whisper model loaded.\")\n",
    "\n",
    "print(\"Transcribing audio with OpenAI Whisper...\")\n",
    "transcription_raw_result = model.transcribe(audio_file, language=\"th\", verbose=False)\n",
    "transcription_result = {'segments': []}\n",
    "if 'segments' in transcription_raw_result:\n",
    "    for segment in transcription_raw_result['segments']:\n",
    "        transcription_result['segments'].append({\n",
    "            'text': segment['text'],\n",
    "            'start': segment['start'],\n",
    "            'end': segment['end']\n",
    "        })\n",
    "print(\"Transcription complete.\")\n",
    "\n",
    "# print(\"\\n--- Transcription Result (raw Whisper output) ---\")\n",
    "# print(f\"Number of transcription segments: {len(transcription_result['segments'])}\")\n",
    "# for i, seg in enumerate(transcription_result['segments']):\n",
    "#     print(f\"  Segment {i+1}: Start={seg['start']:.2f}, End={seg['end']:.2f}, Text='{seg['text'].strip()}'\")\n",
    "# print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "# 3. Forced Alignment with ctc-forced-aligner\n",
    "print(\"Initializing forced alignment service (using load_alignment_model)...\")\n",
    "try:\n",
    "    alignment_model, alignment_tokenizer = load_alignment_model(\n",
    "        device_whisper_align,\n",
    "        dtype=torch.float16 if device_whisper_align == \"cuda\" else torch.float32,\n",
    "    )\n",
    "    print(\"Forced alignment model and tokenizer loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading alignment model: {e}\")\n",
    "    print(\"Please ensure the ctc-forced-aligner library is correctly installed from GitHub and dependencies are met.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "aligned_result_segments = []\n",
    "\n",
    "print(\"Loading full audio for alignment...\")\n",
    "try:\n",
    "    # Using torchaudio.load as it's generally more robust\n",
    "    full_audio_waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    if sample_rate != 16000:\n",
    "        print(f\"Resampling audio from {sample_rate}Hz to 16000Hz...\")\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000).to(device_whisper_align)\n",
    "        full_audio_waveform = resampler(full_audio_waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    # Ensure it's mono and on the correct device for emissions generation\n",
    "    full_audio_waveform = full_audio_waveform.mean(dim=0, keepdim=True).to(device_whisper_align)\n",
    "\n",
    "    # ctc-forced-aligner's generate_emissions expects audio waveform\n",
    "    print(\"Full audio loaded and processed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading/processing audio with torchaudio: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"Performing forced alignment on transcription segments...\")\n",
    "for i, segment in enumerate(transcription_result['segments']):\n",
    "    segment_text = segment['text']\n",
    "    segment_start = segment['start']\n",
    "    segment_end = segment['end']\n",
    "\n",
    "    # print(f\"  Aligning segment {i+1}: {segment_start:.2f}s - {segment_end:.2f}s, Text: '{segment_text.strip()}'\")\n",
    "\n",
    "    # Extract the audio chunk for the current segment\n",
    "    start_sample_abs = int(segment_start * sample_rate)\n",
    "    end_sample_abs = int(segment_end * sample_rate)\n",
    "\n",
    "    start_sample_abs = max(0, start_sample_abs)\n",
    "    end_sample_abs = min(full_audio_waveform.shape[-1], end_sample_abs)\n",
    "\n",
    "    if start_sample_abs >= end_sample_abs:\n",
    "        print(f\"    Warning: Skipping empty or invalid audio chunk for segment {i+1}.\")\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': []\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    current_audio_chunk_tensor = full_audio_waveform[:, start_sample_abs:end_sample_abs]\n",
    "\n",
    "    try:\n",
    "        # Generate emissions for the current audio chunk\n",
    "        emissions, stride = generate_emissions(\n",
    "            alignment_model,\n",
    "            current_audio_chunk_tensor,\n",
    "            batch_size=1 # Process one chunk at a time\n",
    "        )\n",
    "\n",
    "        # Preprocess the text\n",
    "        tokens_starred, text_starred = preprocess_text(\n",
    "            segment_text,\n",
    "            romanize=True, # Set to True if using models that require romanization (e.g., multilingual)\n",
    "            language=\"th\"\n",
    "        )\n",
    "\n",
    "        # Get alignments\n",
    "        segments, scores, blank_token = get_alignments(\n",
    "            emissions,\n",
    "            tokens_starred,\n",
    "            alignment_tokenizer,\n",
    "        )\n",
    "\n",
    "        # Get spans (word-level details)\n",
    "        spans = get_spans(\n",
    "            segments,\n",
    "            text_starred,\n",
    "            offset=0, # This offset is relative to the *chunk*, we'll add segment_start later\n",
    "            blank_token=blank_token\n",
    "        )\n",
    "\n",
    "        # Postprocess results to get word-level timings\n",
    "        aligned_words_raw = postprocess_results(\n",
    "            spans,\n",
    "            score_threshold=0.8 # Adjust threshold if needed\n",
    "        )\n",
    "\n",
    "        aligned_words = []\n",
    "        for word_info in aligned_words_raw:\n",
    "            # Add the original segment's start time to the word timings\n",
    "            word_info['start'] += segment_start\n",
    "            word_info['end'] += segment_start\n",
    "            aligned_words.append(word_info)\n",
    "\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': aligned_words\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not align segment '{segment_text}' (start: {segment_start:.2f}s, end: {segment_end:.2f}s). Error: {e}\")\n",
    "        aligned_result_segments.append({\n",
    "            'text': segment_text,\n",
    "            'start': segment_start,\n",
    "            'end': segment_end,\n",
    "            'words': []\n",
    "        })\n",
    "\n",
    "aligned_result = {'segments': aligned_result_segments}\n",
    "print(\"Alignment complete.\")\n",
    "\n",
    "# print(\"\\n--- Aligned Result ---\")\n",
    "# print(f\"Number of aligned segments: {len(aligned_result['segments'])}\")\n",
    "# for i, seg in enumerate(aligned_result['segments']):\n",
    "#     text_display = \"\".join([word[\"word\"] for word in seg[\"words\"]]).strip() if \"words\" in seg and seg[\"words\"] else seg[\"text\"].strip()\n",
    "#     print(f\"  Segment {i+1}: Start={seg['start']:.2f}, End={seg['end']:.2f}, Text='{text_display}'\")\n",
    "#     if \"words\" in seg and seg[\"words\"]:\n",
    "#         for word in seg[\"words\"]:\n",
    "#             print(f\"    - Word: '{word['word']}', Start={word['start']:.2f}, End={word['end']:.2f}\")\n",
    "# print(\"----------------------\")\n",
    "\n",
    "\n",
    "# 4. Assign Speaker Labels\n",
    "print(\"Assigning speakers to transcription segments...\")\n",
    "result_with_speakers = aligned_result.copy()\n",
    "result_with_speakers['segments'] = []\n",
    "\n",
    "for segment in aligned_result['segments']:\n",
    "    segment_start = segment['start']\n",
    "    segment_end = segment['end']\n",
    "    assigned_speaker = \"Unknown\"\n",
    "\n",
    "    max_overlap = 0\n",
    "    for idx, row in diarization_df.iterrows():\n",
    "        diar_start = row['start']\n",
    "        diar_end = row['end']\n",
    "\n",
    "        overlap_start = max(segment_start, diar_start)\n",
    "        overlap_end = min(segment_end, diar_end)\n",
    "\n",
    "        overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "        if overlap_duration > max_overlap:\n",
    "            max_overlap = overlap_duration\n",
    "            assigned_speaker = row['speaker']\n",
    "\n",
    "    segment['speaker'] = assigned_speaker\n",
    "    result_with_speakers['segments'].append(segment)\n",
    "print(\"Speaker assignment complete.\")\n",
    "\n",
    "\n",
    "# 5. Print the final result\n",
    "print(\"\\n--- Speaker-Labeled Transcription ---\")\n",
    "for segment in result_with_speakers[\"segments\"]:\n",
    "    speaker = segment.get(\"speaker\", \"Unknown Speaker\")\n",
    "    start_time = segment[\"start\"]\n",
    "    end_time = segment[\"end\"]\n",
    "    text = segment[\"text\"].strip()\n",
    "    print(f\"[{speaker}] {start_time:.2f}s - {end_time:.2f}s: {text}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
